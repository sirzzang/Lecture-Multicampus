{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://localhost:8888/notebooks/PJ/Multicampus/Crawl/%ED%81%B4%EB%9E%98%EC%8A%A4%EB%A5%BC%20%EB%A7%8C%EB%93%A4%EC%96%B4%20%EB%B3%B4%EC%9E%90.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 질문\n",
    "> 순서도 따라서 어떻게 해야 되는지\n",
    "\n",
    "1. 날짜 뱉어내\n",
    "2. 날짜를 넘겨\n",
    "3. 트윗 검색 조건 설정해\n",
    "\n",
    "---\n",
    "\n",
    "이건 내가 가능하려나 싶은\n",
    "\n",
    "- getJsondata 안에서 바로 바로 읽어올 때마다 하는 방식은?\n",
    "- 체크포인트,,,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----- 대충 해결------\n",
    "- HTTPError 429 : fakeuseragent 해결 가능한지 확인\n",
    "    - 일단 time.sleep(3) 정도면 트위터가 봐주는듯\n",
    "    - 애초에 여기 헤더에 있는 헤더가 랜덤으로 헤더 가져오고, 거의 fake useragent에 있는 애들이다. 굳이 건들지 않아도 될듯?\n",
    "\n",
    "-------- 해결 ㄴㄴ-----------\n",
    "\n",
    "- 제너레이터 만들어서 날짜 뱉어내고, 하루 단위로 실행하도록 만들기.\n",
    "- 날짜 너무 비효율적인디?\n",
    "- 제발 클래스로...\n",
    "- retry 데코레이터 ㅇㅇㅇ\n",
    "- getJsondata 안에서 바로 바로 읽어올 때마다 하는 방식은?\n",
    "- 체크포인트,,,\n",
    "\n",
    "- 함수 바꾼 부분 정리해/놓기!!!!!\n",
    "- 스태틱 메소드니까 상관 ㄴㄴㄴㄴㄴㄴ\n",
    "- logger 찍어야 하는디\n",
    "\n",
    "\n",
    "-------- adsp 끝나고 ----------\n",
    "\n",
    "\n",
    "- retry while문 구현 실패. 아마 패키지 자체가 tweet 가져오는 과정에서 뭔가 있는듯한데 어떻게 소스 확인해야할지 고민\n",
    "\n",
    "\n",
    "- 패키지 코드 다시 확인 : Tweet 객체 안에서 tqdm 찍게 못 바꾸나.\n",
    "    - 일단 지금까지 확인한 건 tweet 객체 생성을 위해 트위터 스크롤 다 내리는 방식인듯한디.\n",
    "    - 애초에 내가 확인하고 싶은 건 크롤링 진행 상황. 이게 오래 걸리는 건데 밑에서 tqdm 찍는 건 개 빨라서 굳이 의미 ㄴㄴ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: GetOldTweets3 in c:\\users\\sir95\\anaconda3\\envs\\cpu_env\\lib\\site-packages (0.0.11)\n",
      "Requirement already satisfied: lxml>=3.5.0 in c:\\users\\sir95\\anaconda3\\envs\\cpu_env\\lib\\site-packages (from GetOldTweets3) (4.5.1)\n",
      "Requirement already satisfied: pyquery>=1.2.10 in c:\\users\\sir95\\anaconda3\\envs\\cpu_env\\lib\\site-packages (from GetOldTweets3) (1.4.1)\n",
      "Requirement already satisfied: cssselect>0.7.9 in c:\\users\\sir95\\anaconda3\\envs\\cpu_env\\lib\\site-packages (from pyquery>=1.2.10->GetOldTweets3) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "# install GOT3 package\n",
    "!pip install GetOldTweets3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module import\n",
    "\n",
    "import GetOldTweets3 as got\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import urllib\n",
    "import json\n",
    "from time import sleep\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "from random import uniform\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getJsonResponse(tweetCriteria, refreshCursor, cookieJar, proxy, useragent=None, debug=False):\n",
    "    \"\"\"\n",
    "    Invoke an HTTP query to Twitter.\n",
    "    Should not be used as an API function. A static method.\n",
    "    \"\"\"\n",
    "    url = \"https://twitter.com/i/search/timeline?\"\n",
    "\n",
    "    if not tweetCriteria.topTweets:\n",
    "        url += \"f=tweets&\"\n",
    "\n",
    "    url += (\"vertical=news&q=%s&src=typd&%s\"\n",
    "            \"&include_available_features=1&include_entities=1&max_position=%s\"\n",
    "            \"&reset_error_state=false\")\n",
    "\n",
    "    urlGetData = ''\n",
    "\n",
    "    if hasattr(tweetCriteria, 'querySearch'):\n",
    "        urlGetData += tweetCriteria.querySearch\n",
    "\n",
    "    if hasattr(tweetCriteria, 'excludeWords'):\n",
    "        urlGetData += ' -'.join([''] + tweetCriteria.excludeWords)\n",
    "\n",
    "    if hasattr(tweetCriteria, 'username'):\n",
    "        if not hasattr(tweetCriteria.username, '__iter__'):\n",
    "            tweetCriteria.username = [tweetCriteria.username]\n",
    "\n",
    "        usernames_ = [u.lstrip('@') for u in tweetCriteria.username if u]\n",
    "        tweetCriteria.username = {u.lower() for u in usernames_ if u}\n",
    "\n",
    "        usernames = [' from:'+u for u in sorted(tweetCriteria.username)]\n",
    "        if usernames:\n",
    "            urlGetData += ' OR'.join(usernames)\n",
    "\n",
    "    if hasattr(tweetCriteria, 'within'):\n",
    "        if hasattr(tweetCriteria, 'near'):\n",
    "            urlGetData += ' near:\"%s\" within:%s' % (tweetCriteria.near, tweetCriteria.within)\n",
    "        elif hasattr(tweetCriteria, 'lat') and hasattr(tweetCriteria, 'lon'):\n",
    "            urlGetData += ' geocode:%f,%f,%s' % (tweetCriteria.lat, tweetCriteria.lon, tweetCriteria.within)\n",
    "\n",
    "    if hasattr(tweetCriteria, 'since'):\n",
    "        urlGetData += ' since:' + tweetCriteria.since\n",
    "\n",
    "    if hasattr(tweetCriteria, 'until'):\n",
    "        urlGetData += ' until:' + tweetCriteria.until\n",
    "\n",
    "    if hasattr(tweetCriteria, 'minReplies'):\n",
    "        urlGetData += ' min_replies:' + tweetCriteria.minReplies\n",
    "\n",
    "    if hasattr(tweetCriteria, 'minFaves'):\n",
    "        urlGetData += ' min_faves:' + tweetCriteria.minFaves\n",
    "\n",
    "    if hasattr(tweetCriteria, 'minRetweets'):\n",
    "        urlGetData += ' min_retweets:' + tweetCriteria.minRetweets\n",
    "\n",
    "    if hasattr(tweetCriteria, 'lang'):\n",
    "        urlLang = 'l=' + tweetCriteria.lang + '&'\n",
    "    else:\n",
    "        urlLang = ''\n",
    "    url = url % (urllib.parse.quote(urlGetData.strip()), urlLang, urllib.parse.quote(refreshCursor))\n",
    "    useragent = useragent or TweetManager.user_agents[0]\n",
    "\n",
    "    headers = [\n",
    "        ('Host', \"twitter.com\"),\n",
    "        ('User-Agent', useragent),\n",
    "        ('Accept', \"application/json, text/javascript, */*; q=0.01\"),\n",
    "        ('Accept-Language', \"en-US,en;q=0.5\"),\n",
    "        ('X-Requested-With', \"XMLHttpRequest\"),\n",
    "        ('Referer', url),\n",
    "        ('Connection', \"keep-alive\")\n",
    "    ]\n",
    "\n",
    "    if proxy:\n",
    "        opener = urllib.request.build_opener(urllib.request.ProxyHandler({'http': proxy, 'https': proxy}), urllib.request.HTTPCookieProcessor(cookieJar))\n",
    "    else:\n",
    "        opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cookieJar))\n",
    "    opener.addheaders = headers\n",
    "\n",
    "    if debug:\n",
    "        print(url)\n",
    "        # print('\\n'.join(h[0]+': '+h[1] for h in headers))\n",
    "\n",
    "    # HTTP Request 429 : 타임 슬립 줘보자\n",
    "    time.sleep(3)\n",
    "\n",
    "    try:\n",
    "        response = opener.open(url)\n",
    "        jsonResponse = response.read()\n",
    "\n",
    "    except TimeoutError as e:\n",
    "        print(\"Timeout error\")\n",
    "        print('sleep 30')\n",
    "        time.sleep(30)\n",
    "\n",
    "        #이 부분 나중에 retry로 진행하기\n",
    "        try:\n",
    "            response = opener.open(url)\n",
    "            jsonResponse = response.read()\n",
    "        except TimeoutError as e:\n",
    "            print(\"Timeout Error again.\")\n",
    "            print(\"Pass Data!!!!!!!!!\")\n",
    "            pass\n",
    "\n",
    "#     count = 0\n",
    "#     while True:\n",
    "#         try:\n",
    "#             response = opener.open(url)\n",
    "#             jsonResponse = response.read()\n",
    "        \n",
    "#             # 완료시 break 에러시 except 문 실행.\n",
    "#             break\n",
    "            \n",
    "#         except TimeOutError as e:\n",
    "#             print(\"Timeout error\")\n",
    "#             count+=1\n",
    "#             if count >=3:\n",
    "#                 break\n",
    "#             time.sleep(30)\n",
    "\n",
    "#     if count >=3:\n",
    "#         continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occured during an HTTP request:\", str(e))\n",
    "        print(\"Error URL:\", url)\n",
    "        print(\"Try to open in browser: https://twitter.com/search?q=%s&src=typd\" % urllib.parse.quote(urlGetData))\n",
    "\n",
    "        # 이 부분 나중에 확인하기\n",
    "        print(\"sleep 30!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        pass\n",
    "        # sys.exit()\n",
    "        \n",
    "\n",
    "    try:\n",
    "        s_json = jsonResponse.decode()\n",
    "    except:\n",
    "        print(\"Invalid response from Twitter\")\n",
    "        print(\"Error URL:\", url)\n",
    "        pass\n",
    "        # sys.exit()\n",
    "    else:\n",
    "        try:\n",
    "            dataJson = json.loads(s_json)\n",
    "        except:\n",
    "            print(\"Error parsing JSON: %s\" % s_json)\n",
    "            print(\"Error URL:\", url)\n",
    "        pass\n",
    "        # sys.exit()\n",
    "\n",
    "#     if debug:\n",
    "#         print(s_json)\n",
    "#         print(\"---\\n\")\n",
    "\n",
    "    return dataJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜 다시 입력해야 할 때 에러 발생시키자.\n",
    "class NotValidEndDateError(Exception):\n",
    "    def __init__(self):\n",
    "        super().__init__('마지막 검색 날짜를 다시 설정하십시오.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GetOldTweets3 패키지의 setUntil 메소드는 마지막 날짜 포함하지 않으므로, 주의.\n",
    "def set_crawl_date(start_date, end_date):\n",
    "    \n",
    "    start_date = datetime.datetime.strptime(str(start_date), \"%Y%m%d\")\n",
    "    end_date = datetime.datetime.strptime(str(end_date), \"%Y%m%d\") - datetime.timedelta(days=1)\n",
    "    \n",
    "    if end_date == start_date:\n",
    "        raise NotValidEndDateError\n",
    "    \n",
    "    else:   \n",
    "        print(\"트윗 수집 날짜 설정: {0}부터 {1}까지\".format(start_date, end_date))    \n",
    "        return start_date.strftime(\"%Y-%m-%d\"), end_date.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_tweets(start_date, end_date, debug=False):    \n",
    "    got.manager.TweetManager.getJsonResponse = getJsonResponse # 함수에 이 부분 넣어주고\n",
    "    \n",
    "    print(\"========== 트윗 수집 시작: {0} ~ {1} ==========\".format(start_date, end_date))\n",
    "    start_time = time.time()\n",
    "    tweet_criteria = got.manager.TweetCriteria().setQuerySearch('Elon Musk')\\\n",
    "                                                .setSince(start_date)\\\n",
    "                                                .setUntil(end_date)\\\n",
    "                                                .setLang('en')\n",
    "    tweets = got.manager.TweetManager.getTweets(tweet_criteria, debug=debug)\n",
    "    \n",
    "    elapsed_time = time.time()-start_time\n",
    "    \n",
    "    print(\"수집 완료 : {}\".format(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))))\n",
    "    print(\"총 수집 트윗 개수 : {0}\".format(len(tweets)))\n",
    "    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(tweet_data):\n",
    "    results = []\n",
    "    for tweet in tqdm(tweet_data):\n",
    "        results.append({'url': tweet.permalink,\n",
    "                        'date': tweet.date,\n",
    "                        'text': tweet.text,\n",
    "                        'user': tweet.username,\n",
    "                        'mentions': tweet.mentions,\n",
    "                        'retweets': tweet.retweets,\n",
    "                        'favorites': tweet.favorites,\n",
    "                        'hashtags': tweet.hashtags})\n",
    "        \n",
    "    return results        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tweets(tweet_lists):\n",
    "    base_file_dir = \"tweets\"\n",
    "\n",
    "    if not os.path.exists(base_file_dir):\n",
    "        os.makedirs(base_file_dir)\n",
    "        \n",
    "    with open(f\"{base_file_dir}/tweets_{crawl_start}_{crawl_end}.csv\", \"a\", -1, encoding=\"utf-8\") as f:    \n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['url', 'date', 'text', 'user', 'mentions', 'retweets', 'favorites', 'hashtags'])        \n",
    "        for tweet_list in tqdm(tweet_lists):\n",
    "            writer.writerow(list(tweet_list.values()))\n",
    "            \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트윗 수집 날짜 설정: 2020-02-01 00:00:00부터 2020-02-02 00:00:00까지\n"
     ]
    }
   ],
   "source": [
    "crawl_start, crawl_end = set_crawl_date(20200201, 20200203)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 트윗 수집 시작: 2020-02-01 ~ 2020-02-02 ==========\n",
      "수집 완료 : 01:00:26\n",
      "총 수집 트윗 개수 : 11232\n"
     ]
    }
   ],
   "source": [
    "tweet_results = crawl_tweets(crawl_start, crawl_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 11232/11232 [00:00<00:00, 196353.15it/s]\n"
     ]
    }
   ],
   "source": [
    "tweet_results_lists = get_results(tweet_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 11232/11232 [00:00<00:00, 49620.79it/s]\n"
     ]
    }
   ],
   "source": [
    "save_tweets(tweet_results_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트윗 수집 날짜 설정: 2020-06-01 00:00:00부터 2020-06-02 00:00:00까지\n"
     ]
    }
   ],
   "source": [
    "crawl_start, crawl_end = set_crawl_date(20200601, 20200603)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 트윗 수집 시작: 2020-06-01 ~ 2020-06-02 ==========\n",
      "수집 완료 : 01:31:45\n",
      "총 수집 트윗 개수 : 22646\n"
     ]
    }
   ],
   "source": [
    "tweet_results = crawl_tweets(crawl_start, crawl_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 22646/22646 [00:00<00:00, 509678.57it/s]\n"
     ]
    }
   ],
   "source": [
    "tweet_results_lists = get_results(tweet_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 22646/22646 [00:00<00:00, 105594.68it/s]\n"
     ]
    }
   ],
   "source": [
    "save_tweets(tweet_results_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# retry 질문..\n",
    "\n",
    "도대체 어떻게 쓰는거니 ㅠㅅㅠ 머리야 생각을해..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def retry(total_try_cnt=5, sleep_in_sec=5, retryable_exceptions=(TimeoutError)):\n",
    "#     def decorator(getJsonResponse):\n",
    "#         @functools.wraps(getJsonResponse)\n",
    "#         def wrapper(tweetCriteria, refreshCursor, cookieJar, proxy, useragent=None, debug=False):\n",
    "#             for cnt in range(total_try_cnt):\n",
    "#                 logger.info(f\"trying {func.__name__}() [{cnt+1}/{total_try_cnt}]\")\n",
    "\n",
    "#                 try:\n",
    "#                     result = func(*args, **kwargs)\n",
    "#                     logger.info(f\"in retry(), {func.__name__}() returned '{result}'\")\n",
    "\n",
    "#                     if result: \n",
    "#                         return result\n",
    "#                 except retryable_exceptions as e:\n",
    "#                     logger.info(f\"in retry(), {func.__name__}() raised retryable exception '{e}'\")\n",
    "#                     pass\n",
    "#                 except Exception as e:\n",
    "#                     logger.info(f\"in retry(), {func.__name__}() raised {e}\")\n",
    "#                     print(str(e))\n",
    "#                     pass \n",
    "#                     # raise e\n",
    "                    \n",
    "\n",
    "#                 time.sleep(sleep_in_sec)\n",
    "#             logger.info(f\"{func.__name__} finally has been failed\")\n",
    "#         return wrapper\n",
    "#     return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NameError: name 'tweetCriteria' is not defined\n",
    "\n",
    "--> 안에 넣어줘도 안 된다고 나온다...\n",
    "\n",
    "\n",
    "`crawl_tweets(crawl_start, crawl_end)` 진행 시 오류\n",
    "\n",
    "TypeError                                 Traceback (most recent call last)\n",
    "<ipython-input-29-e4d276fc0ef7> in <module>()\n",
    "----> 1 tweet_results = crawl_tweets(crawl_start, crawl_end)\n",
    "\n",
    "TypeError: getJsonResponse() missing 2 required positional arguments: 'cookieJar' and 'proxy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @retry(total_try_cnt=3, sleep_in_sec=3, retryable_exceptions=(TimeoutError))\n",
    "# def my_method(tweetCriteria, refreshCursor, cookieJar, proxy, useragent=None, debug=False):\n",
    "#     rand = random.randint(0, 9)\n",
    "#     logger.info(f\"rand={rand}\")\n",
    "\n",
    "#     if rand % 3 == 0:\n",
    "#         return True\n",
    "#     elif rand % 3 == 1:\n",
    "#         return False\n",
    "#     else:\n",
    "#         raise int(\"a\")\n",
    "\n",
    "# my_method(tweetCriteria, refreshCursor, cookieJar, proxy, useragent=None, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def retry(total_try_cnt=3, error_sleep=30, request_delay=2, retryable_exceptions=()):\n",
    "    \n",
    "#     def decorator(getJsonResponse):\n",
    "#         @functools.wraps(getJsonResponse(tweetCriteria, refreshCursor, cookieJar, proxy, useragent=None, debug=False))\n",
    "#         def getJsonResponse(tweetCriteria, refreshCursor, cookieJar, proxy, useragent=None, debug=False):\n",
    "#             \"\"\"\n",
    "#             Invoke an HTTP query to Twitter.\n",
    "#             Should not be used as an API function. A static method.\n",
    "#             \"\"\"\n",
    "            \n",
    "#             url = \"https://twitter.com/i/search/timeline?\"\n",
    "\n",
    "#             # URL\n",
    "\n",
    "#             if not tweetCriteria.topTweets:\n",
    "#                 url += \"f=tweets&\"\n",
    "\n",
    "#             url += (\"vertical=news&q=%s&src=typd&%s\"\n",
    "#                     \"&include_available_features=1&include_entities=1&max_position=%s\"\n",
    "#                     \"&reset_error_state=false\")\n",
    "\n",
    "#             urlGetData = ''\n",
    "\n",
    "#             if hasattr(tweetCriteria, 'querySearch'):\n",
    "#                 urlGetData += tweetCriteria.querySearch\n",
    "\n",
    "#             if hasattr(tweetCriteria, 'excludeWords'):\n",
    "#                 urlGetData += ' -'.join([''] + tweetCriteria.excludeWords)\n",
    "\n",
    "#             if hasattr(tweetCriteria, 'username'):\n",
    "#                 if not hasattr(tweetCriteria.username, '__iter__'):\n",
    "#                     tweetCriteria.username = [tweetCriteria.username]\n",
    "\n",
    "#                 usernames_ = [u.lstrip('@') for u in tweetCriteria.username if u]\n",
    "#                 tweetCriteria.username = {u.lower() for u in usernames_ if u}\n",
    "\n",
    "#                 usernames = [' from:'+u for u in sorted(tweetCriteria.username)]\n",
    "#                 if usernames:\n",
    "#                     urlGetData += ' OR'.join(usernames)\n",
    "\n",
    "#             if hasattr(tweetCriteria, 'within'):\n",
    "#                 if hasattr(tweetCriteria, 'near'):\n",
    "#                     urlGetData += ' near:\"%s\" within:%s' % (tweetCriteria.near, tweetCriteria.within)\n",
    "#                 elif hasattr(tweetCriteria, 'lat') and hasattr(tweetCriteria, 'lon'):\n",
    "#                     urlGetData += ' geocode:%f,%f,%s' % (tweetCriteria.lat, tweetCriteria.lon, tweetCriteria.within)\n",
    "\n",
    "#             if hasattr(tweetCriteria, 'since'):\n",
    "#                 urlGetData += ' since:' + tweetCriteria.since\n",
    "\n",
    "#             if hasattr(tweetCriteria, 'until'):\n",
    "#                 urlGetData += ' until:' + tweetCriteria.until\n",
    "\n",
    "#             if hasattr(tweetCriteria, 'minReplies'):\n",
    "#                 urlGetData += ' min_replies:' + tweetCriteria.minReplies\n",
    "\n",
    "#             if hasattr(tweetCriteria, 'minFaves'):\n",
    "#                 urlGetData += ' min_faves:' + tweetCriteria.minFaves\n",
    "\n",
    "#             if hasattr(tweetCriteria, 'minRetweets'):\n",
    "#                 urlGetData += ' min_retweets:' + tweetCriteria.minRetweets\n",
    "\n",
    "#             if hasattr(tweetCriteria, 'lang'):\n",
    "#                 urlLang = 'l=' + tweetCriteria.lang + '&'\n",
    "#             else:\n",
    "#                 urlLang = ''\n",
    "            \n",
    "#             url = url % (urllib.parse.quote(urlGetData.strip()), urlLang, urllib.parse.quote(refreshCursor))\n",
    "#             useragent = useragent or TweetManager.user_agents[0]\n",
    "\n",
    "#             headers = [\n",
    "#                 ('Host', \"twitter.com\"),\n",
    "#                 ('User-Agent', useragent),\n",
    "#                 ('Accept', \"application/json, text/javascript, */*; q=0.01\"),\n",
    "#                 ('Accept-Language', \"en-US,en;q=0.5\"),\n",
    "#                 ('X-Requested-With', \"XMLHttpRequest\"),\n",
    "#                 ('Referer', url),\n",
    "#                 ('Connection', \"keep-alive\")\n",
    "#             ]\n",
    "\n",
    "#             if proxy:\n",
    "#                 opener = urllib.request.build_opener(urllib.request.ProxyHandler({'http': proxy, 'https': proxy}), urllib.request.HTTPCookieProcessor(cookieJar))\n",
    "#             else:\n",
    "#                 opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cookieJar))\n",
    "#             opener.addheaders = headers\n",
    "\n",
    "#             if debug:\n",
    "#                 print(url)\n",
    "#                 # print('\\n'.join(h[0]+': '+h[1] for h in headers))\n",
    "\n",
    "            \n",
    "#             # HTTP Request 429 : 타임 슬립 줘보자\n",
    "#             time.sleep(request_delay)\n",
    "\n",
    "#             # JSON으로 정보 받아 오기\n",
    "\n",
    "#             for cnt in range(total_try_cnt):\n",
    "\n",
    "#                 logger.info(f\"{getJsonResponse.__name__}() : {cnt+1} / {total_try_cnt}\")\n",
    "                \n",
    "#                 try:\n",
    "#                     response = opener.open(url)\n",
    "#                     jsonResponse = response.read()\n",
    "#                     logger.info(f\"{getJsonResponse.__name__}() : {url}에서 데이터를 추출했습니다.\")\n",
    "\n",
    "#                 except retryable_exceptions as re:\n",
    "#                     print(f\"{re}: sleep {error_sleep}\")\n",
    "#                     time.sleep(error_sleep)                    \n",
    "#                     logger.info(f\"다시 시도: {getJsonResponse.__name__}()에서 {re}가 발생했습니다.\")\n",
    "                \n",
    "#                 try:\n",
    "#                     response = opener.open(url)\n",
    "#                     jsonResponse = response.read()\n",
    "                \n",
    "\n",
    "#                 # except TimeoutError as e:\n",
    "#                 #     print(\"Timeout error\")\n",
    "#                 #     print('sleep 30')\n",
    "#                 #     time.sleep(30)\n",
    "\n",
    "#                 #     # 이 부분 나중에 retry로 진행하기\n",
    "#                 #     try:\n",
    "#                 #         response = opener.open(url)\n",
    "#                 #         jsonResponse = response.read()\n",
    "#                 #     except TimeoutError as e:\n",
    "#                 #         print(\"Timeout Error again.\")\n",
    "#                 #         print(\"Pass Data\")\n",
    "#                 #         pass\n",
    "\n",
    "#                 except Exception as e:\n",
    "#                     print(\"An error occured during an HTTP request:\", str(e))\n",
    "#                     print(\"Try to open in browser: https://twitter.com/search?q=%s&src=typd\" % urllib.parse.quote(urlGetData))\n",
    "#                     logger.info(f\"{getJsonResponse.__name__}(): {e} 발생, url: {url}\")\n",
    "#                     # 이 부분 나중에 확인하기\n",
    "#                     print(\"sleep {error_sleep} and pass data\")\n",
    "#                     time.sleep(error_sleep)\n",
    "#                     pass\n",
    "#                     # sys.exit()                   \n",
    "                    \n",
    "#                 try:\n",
    "#                     s_json = jsonResponse.decode()\n",
    "#                 except:\n",
    "#                     logger.info(f\"{getJsonResponse.__name__}(): Invalid response from Twitter\")\n",
    "#                     print(\"Invalid response from Twitter\")\n",
    "#                     pass\n",
    "#                     # sys.exit()                    \n",
    "#                 else:\n",
    "#                     try:\n",
    "#                         dataJson = json.loads(s_json)\n",
    "#                         # return dataJson\n",
    "#                     except:\n",
    "#                         logger.info(f\"{getJsonResponse.__name__}(): Json Parsing 에러, url: {url}\")\n",
    "#                         print(\"Error parsing JSON: %s\" % s_json)\n",
    "#                         pass\n",
    "#                         # sys.exit()\n",
    "                \n",
    "#                 if dataJson:\n",
    "#                     return dataJson\n",
    "\n",
    "#             logger.info(f\"{func.__name__}() : FAILED\")\n",
    "        \n",
    "#         return getJsonResponse\n",
    "#     return decorator\n",
    "\n",
    "# # @retry(total_try_cnt=3, )\n",
    "\n",
    "\n",
    "#                 # return dataJson\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @retry(total_try_cnt=3, error_sleep=20, request_delay=2, retryable_exceptions=(TimeoutError))\n",
    "# def crawl_tweets(start_date, end_date):    \n",
    "#     got.manager.TweetManager.getJsonResponse = getJsonResponse # (tweetCriteria, refreshCursor, cookieJar, proxy, useragent=None, debug=False) # 함수에 이 부분 넣어주고\n",
    "    \n",
    "#     print(\"========== 트윗 수집 시작: {0} ~ {1} ==========\".format(start_date, end_date))\n",
    "#     start_time = time.time()\n",
    "#     tweet_criteria = got.manager.TweetCriteria().setQuerySearch('Elon Musk')\\\n",
    "#                                                 .setSince(start_date)\\\n",
    "#                                                 .setUntil(end_date)\\\n",
    "#                                                 .setLang('en')\n",
    "#     tweets = got.manager.TweetManager.getTweets(tweet_criteria, debug=True)\n",
    "    \n",
    "#     elapsed_time = time.time()-start_time\n",
    "    \n",
    "#     print(\"수집 완료 : {}\".format(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))))\n",
    "#     print(\"총 수집 트윗 개수 : {0}\".format(len(test_tweet)))\n",
    "    \n",
    "#     return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
