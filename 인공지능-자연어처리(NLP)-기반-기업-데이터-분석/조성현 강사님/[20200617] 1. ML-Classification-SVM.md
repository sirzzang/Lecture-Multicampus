# [머신러닝]



딥러닝 이전 유명했던 알고리즘. KNN이나 Decision Tree나 의사결정선을 찾는 게 문제. 오른쪽 파란색 점이 클래스2, 빨간색 점이 클래스1. 의사결정선을 A, B의 두 개로 생각해볼 수 있다. A 선을 보면 클래스 1과 클래스 2를 완벽하게 구분하고 있다. decision boundary 역할을 하고 있다. 

 그러나 둘 중 B가 낫다. A는 조금만 이동해도 닿을 것 같고, 왜인지 모르게 불안해 보인다. 반면 B는 넉넉해 보인다. 어디로 가도 공간이 많아! B가 더 안전해 보여. A 같은 바운더리 말고, B와 같은 바운더리를 찾는 알고리즘을 서포트 벡터 머신이라고 !

 왜 B가 더 안전한가? 어느 쪽으로나 여유가 많다. 그래서 B가 더 좋아 보인다. B를 오른쪽 왼쪽으로 이동시켜 보자. 이동시키다 보면 최초 데이터와 만나는 지점이. 점선으로 표시할 수 있는데, 점선까지의 거리가 A보다는 크고, 



 이 지지선들. 

 최초로 만나는 벡터를 서포트 벡터라고 한다. 데이터는 공간 상에서 전부 벡터다. 좌표로 표현된다. 서포트 벡터 머신.

 개요는 단순한데, 이 직선을 찾으려면 수학적으로 어려운 과정을 거쳐야 합니당...



 수학적 과정을 구경하고, 너무 깊이 몰두하지는 맙시다.



일단 원리를 살펴 보죠.



 사진 넣고!





수학적으로 명확하게는 보지 않고 직관적으로만 이해합시다.



svm은 기본적으로 이진분류

클래스가 여러 개 있는 경우라면, 하나를 +1이라고 놓고 나머지를 -1이라고 놔서 구분하는 직선을 찾고. 그 다음에 같은 과정 반복해서 나눈다. OVR 방식. 







선형분리가 불가능한 경우!



아. 그런데 섞이는 애가 있을 수밖에 없다. 그래서 slack변수를 추가한다.

slack 변수를 추가해서 직선의 방정식을 찾는다. 왜?? 여기 놓쳤다.

마진을 맥시마이즈할 것이냐 섞인 비율을 최소화할 것이냐. 마진이 작아지더라도 두 개 중간 정도 되는 상황에서 의사결정할 수 있다. 어쩔 수 없는 상황인데, 양보를 한다. 슬랙 변수 추가해서 목표 함수를 잡는다.

마진을 최대화하기 위한 조건은 그대로 쓰고, 섞여 있는 것을 인정해야 하기 때문에 슬랙 변수를 목표에 추가한다. 중간적인 특성이 나오도록.

오른쪽이 마진을 극대화하는 쪽에 대한 일종의 벌점항.  C를 크게 준다는 얘기는 마진은 신경 안 쓰고 섞이는 걸 최소화한다는 의미. C를 작게 준다는 얘기는 마진을 극대화한다.

 C가 조절변수의 역할을.



 그렇게 한 후 라그랑지안을 하고! 라그랑지안의 합이 0보다 크다는 애가 제약조건이 추가된다. 두 개의 제약조건이기 때문에, 라그랑지안 승수가 두 개 필요함. 람다와 뮤. 듀얼 라그랑지안 식을 만들어 내면 되는데 슬랙 변수가 없는 듀얼 라그랑지안과 똑같다. 아까와 같은 과정으로 람다를 풀고, w와 d결정.



> 앞의 경우는 하드 마진이라는 말을 쓰고, 섞여 있어서 명백히 나오기 어려운 경우는 소프트 마진이라고 한다. 



  소프트 마진의 경우 중간 지점 어떻게 결정할지.







  훈련 데이터가 많은 경우 전부 다 찾기 어렵기 때문에, QP 알고리즘을 사용한다. Quadratic









 경계가 곡선으로 그려져야 하는 경우는?



 공간을 다른 공간으로 나누면 선형 분리가 가능해질 수도 있다. 

종이를 구겨서 만든다고 생각하자!





 그러면 2차웡늬 공간을 또 다른 어떤 공간으로 변환하는 것만 결정할 수 있떠라도 가능. kernel function이라고 합니다. 고난도. 어쨌든 원리만 본다면.

 알고 있는 각각 데이터의 내적을 이용해서 공간을 변형하면, 어차피 라그랑지 식이 필요한 거니까, 그런 커널 함수를 쓰면 곡선으로 분할할 수 있다! 그게 커널 트릭이라는 알고리즘. 



SVM 너무 깊이 가지는 마세요.

비선형인 애들을 선형처럼 만들어주려고 커널 트릭을 쓴다. 공간변환을 해야 하는데, 공간변환의 형태가 어떠한지 모른다. 일단 공간변환 트릭을 써서 데이터 벡터 한 번에 내적을 해서 공간변환을 하나, 각각 변환을 해서 내적을 하나 같으면 그 커널을 사용하기로 한다. 

선형 분포가 아닌 애를 분리하려고 하다 보니 나온 트릭. 다 옛날의 이야기입니다.

딥러닝이 나오면서 내부적으로 그게 다 됩니당.

딥러닝은 어차피 태생적으로 비선형을 다루기 위해 태어난 학문이기 때문에, 머신러닝으로 비선형을 다루려고 하는 건 이전의 이론! 





 사이킷런에서는 SVC 함수를 사용합니다. 선형분리일 때는 일단 커널 리니어로 모델 빌드하고, 학습 데이터로 fit. fit 과정에서 람다 QT 알고리즘으로 찾아 내고. 늘 했던 대로 테스트 데이터를 넣어서 성능을 평가하면 된다. 카테고리가 3개이므로 내부적으로 2번 수행한다.





실습 시 시각화를 위해 `mglearn` 패키지를 이용한다. 3차원을 시각화하기 어렵기 때문에, 2개의 feature(sepal_length, sepal_width)만 사용한다.



c 조정하면서 확인한 결과, 이 경우에는 c 값의 변화에 크게 모델이 영향을 받지 않는 것 같다. 그 정도로 이해하고 넘어갑시다.







비서녛ㅇ

rbf

radial basis function



gamma를 크게 주면 분포가 뾰족하다. 감마도 사람이 지정해주어야 할 변수. 슬랙 변수인 C도 사람이 지정해주어야.

비선형 시 감마와 C의 두 가지 변수를 지정해주어야 한다.





